{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32773879-cb67-4c4e-8328-e330026242bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "from helpers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unet_model import multi_unet_model, jacard_coef  \n",
    "from skimage.io import imread, imshow\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from patchify import patchify\n",
    "from PIL import Image, ImageOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd72e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d16e1df-427d-40d3-b574-a61486d31d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "tf.random.set_seed(42) # To load models \n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b913e28a-ab06-4119-82af-c0f6705dd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIRECTORY = 'training/'\n",
    "PATCH_SIZE = 256\n",
    "NUMBER_IMAGES_TRAINING = 100\n",
    "TRAIN_PATH_IMAGES = 'training/images/'\n",
    "TRAIN_PATH_GROUNDTRUTH = 'training/groundtruth/'\n",
    "TEST_PATH = 'test_set_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096d1f7-84bd-43a5-b4a7-e11e1cee6964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read images from repsective 'images' subdirectory\n",
    "#As all images are of ddifferent size we have 2 options, either resize or crop\n",
    "#But, some images are too large and some small. Resizing will change the size of real objects.\n",
    "#Therefore, we will crop them to a nearest size divisible by 256 and then \n",
    "#divide all images into patches of 256x256x3. \n",
    "image_dataset = []  \n",
    "position = 0\n",
    "for path, subdirs, files in os.walk(ROOT_DIRECTORY):\n",
    "    dirname = path.split(\"/\")[-1]\n",
    "    if dirname == 'images':   #Find all 'images' directories\n",
    "        images = os.listdir(path)  #List of all image names in this subdirectory\n",
    "        for i, image_name in tqdm(enumerate(images), total=NUMBER_IMAGES_TRAINING):\n",
    "            if i+1 > NUMBER_IMAGES_TRAINING:\n",
    "                break\n",
    "            if image_name.endswith(\".png\"):   #Only read jpg images...\n",
    "               \n",
    "                image = cv2.imread(path+\"/\"+image_name, 1)  #Read each image as BGR\n",
    "                SIZE_X = (image.shape[1]//PATCH_SIZE)*PATCH_SIZE #Nearest size divisible by our patch size\n",
    "                SIZE_Y = (image.shape[0]//PATCH_SIZE)*PATCH_SIZE #Nearest size divisible by our patch size\n",
    "                image = Image.fromarray(image)\n",
    "                image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "                #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "                image = np.array(image) \n",
    "                #Extract patches from each image\n",
    "                #print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
    "                patches_img = patchify(image, (PATCH_SIZE, PATCH_SIZE, 3), step=PATCH_SIZE)  #Step=256 for 256 patches means no overlap\n",
    "        \n",
    "                for i in range(patches_img.shape[0]):\n",
    "                    for j in range(patches_img.shape[1]):\n",
    "                        \n",
    "                        single_patch_img = patches_img[i,j,:,:]\n",
    "                        \n",
    "                        #Use minmaxscaler instead of just dividing by 255. \n",
    "                        \n",
    "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. \n",
    "                        single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds. \n",
    "                        image_dataset.append(single_patch_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bc59cd-5454-47bf-b98b-9897c94bac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dataset = []  \n",
    "position = 0\n",
    "for path, subdirs, files in os.walk(ROOT_DIRECTORY):\n",
    "    #print(path)  \n",
    "    dirname = path.split(\"/\")[-1]\n",
    "    if dirname == 'groundtruth':   #Find all 'images' directories\n",
    "        masks = os.listdir(path)  #List of all image names in this subdirectory\n",
    "        for i, mask_name in tqdm(enumerate(masks), total=NUMBER_IMAGES_TRAINING):  \n",
    "            if i+1 > NUMBER_IMAGES_TRAINING:\n",
    "                break\n",
    "            if mask_name.endswith(\".png\"):   #Only read png images... (masks in this dataset)\n",
    "               \n",
    "                mask = cv2.imread(path+\"/\"+mask_name, 1)  #Read each image as Grey (or color but remember to map each color to an integer)\n",
    "                mask = cv2.cvtColor(mask,cv2.COLOR_BGR2RGB)\n",
    "                SIZE_X = (mask.shape[1]//PATCH_SIZE)*PATCH_SIZE #Nearest size divisible by our patch size\n",
    "                SIZE_Y = (mask.shape[0]//PATCH_SIZE)*PATCH_SIZE #Nearest size divisible by our patch size\n",
    "                mask = Image.fromarray(mask)\n",
    "                mask = mask.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "                #mask = mask.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "                mask = np.array(mask)             \n",
    "       \n",
    "                #Extract patches from each image\n",
    "                #print(\"Now patchifying mask:\", path+\"/\"+mask_name)\n",
    "                patches_mask = patchify(mask, (PATCH_SIZE, PATCH_SIZE, 1), step=PATCH_SIZE)  #Step=256 for 256 patches means no overlap\n",
    "        \n",
    "                for i in range(patches_mask.shape[0]):\n",
    "                    for j in range(patches_mask.shape[1]):\n",
    "                        \n",
    "                        single_patch_mask = patches_mask[i,j,:,:]\n",
    "                        #single_patch_img = (single_patch_img.astype('float32')) / 255. #No need to scale masks, but you can do it if you want\n",
    "                        single_patch_mask = single_patch_mask[0] #Drop the extra unecessary dimension that patchify adds.   \n",
    "                        mask_dataset.append(single_patch_mask) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bc30b8-a843-4e82-98b3-f3102bbe3a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test images\n",
    "test_ids = os.listdir(TEST_PATH)\n",
    "test_dataset = []\n",
    "print('Resizing test images') \n",
    "for n, id_ in tqdm(enumerate(test_ids), total=len(test_ids)):\n",
    "    path = TEST_PATH + id_\n",
    "    image = Image.open(path+\"/\"+ id_+\".png\")\n",
    "    new_size_X = math.ceil(image.size[1]/PATCH_SIZE)*PATCH_SIZE\n",
    "    new_size_Y = math.ceil(image.size[0]/PATCH_SIZE)*PATCH_SIZE\n",
    "    image = resize_with_padding(image, (new_size_X, new_size_Y))\n",
    "    image = np.array(image)  #Read each image as BGR\n",
    "    #print(image.shape)\n",
    "    SIZE_X = (image.shape[1]//PATCH_SIZE)*PATCH_SIZE #Nearest size divisible by our patch size\n",
    "    SIZE_Y = (image.shape[0]//PATCH_SIZE)*PATCH_SIZE #Nearest size divisible by our patch size\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.crop((0 ,0, SIZE_X, SIZE_Y))  #Crop from top left corner\n",
    "    #image = image.resize((SIZE_X, SIZE_Y))  #Try not to resize for semantic segmentation\n",
    "    image = np.array(image) \n",
    "    #Extract patches from each image\n",
    "    #print(\"Now patchifying image:\", path+\"/\"+image_name)\n",
    "    patches_img = patchify(image, (PATCH_SIZE, PATCH_SIZE, 3), step=PATCH_SIZE)  #Step=256 for 256 patches means no overlap\n",
    "\n",
    "    for i in range(patches_img.shape[0]):\n",
    "        for j in range(patches_img.shape[1]):\n",
    "\n",
    "            single_patch_img = patches_img[i,j,:,:]\n",
    "\n",
    "            #Use minmaxscaler instead of just dividing by 255. \n",
    "            #single_patch_img = scaler.fit_transform(single_patch_img.reshape(-1, single_patch_img.shape[-1])).reshape(single_patch_img.shape)\n",
    "\n",
    "            #single_patch_img = (single_patch_img.astype('float32')) / 255. \n",
    "            single_patch_img = single_patch_img[0] #Drop the extra unecessary dimension that patchify adds. \n",
    "            test_dataset.append(single_patch_img)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93d4e0-d172-4de3-8815-b97dd3d3f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = np.array(image_dataset)\n",
    "mask_dataset = np.array(mask_dataset, dtype=np.bool_)\n",
    "test_dataset = np.array(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fc7fb9-6eed-40d6-a022-85a75c66425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_dataset.shape)\n",
    "print(mask_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312f2b43-4c52-4811-a51b-aa44dd886d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = random.randint(0, len(image_dataset))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(image_dataset[image_number], (PATCH_SIZE, PATCH_SIZE, 3)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(mask_dataset[image_number], (PATCH_SIZE, PATCH_SIZE, 1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640847d4-7184-42b7-8745-12bc17a4b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(image_dataset, mask_dataset, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a6d908-968f-44b5-8d34-2f69ea0c8dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20807fb2-402f-4da1-9bc8-1daca970e832",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IMG_HEIGHT = X_train.shape[1]\n",
    "IMG_WIDTH  = X_train.shape[2]\n",
    "IMG_CHANNELS = X_train.shape[3]\n",
    "\n",
    "metrics=['accuracy', jacard_coef]\n",
    "\n",
    "def get_model():\n",
    "    return multi_unet_model(IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS)\n",
    "\n",
    "model = get_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0538b19e-6dbf-4cfa-85aa-b926f13f99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"check_points/unet_model_1\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "        tf.keras.callbacks.TensorBoard(log_dir='logs'),\n",
    "        tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_weights_only=True,verbose=1)]\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "            batch_size = 16, \n",
    "            verbose=1, \n",
    "            epochs=6, \n",
    "            validation_data=(X_test, y_test), \n",
    "            shuffle=False,\n",
    "            callbacks=callbacks)\n",
    "\n",
    "model.save(\"models/unet_model_1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aac690-08d8-420a-9ad4-8cb98838c1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_objects = {\"jacard_coef\": jacard_coef}\n",
    "with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    model = tf.keras.models.load_model('models/unet_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037adb62-3200-4b56-9866-1373d20bb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = model.predict(image_dataset[:int(image_dataset.shape[0]*0.9)], verbose=1)\n",
    "preds_val = model.predict(image_dataset[int(image_dataset.shape[0]*0.9):], verbose=1)\n",
    "preds_test = model.predict(test_dataset, verbose=1)\n",
    "\n",
    " \n",
    "preds_train_t = (preds_train > 0.5).astype(np.uint8)\n",
    "preds_val_t = (preds_val > 0.5).astype(np.uint8)\n",
    "preds_test_t = (preds_test > 0.5).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05507f95-3c33-4639-8abf-9945733fe4a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Perform a sanity check on some random training samples\n",
    "ix = random.randint(0, len(preds_train_t))\n",
    "imshow(image_dataset[ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(mask_dataset[ix]))\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_train_t[ix]))\n",
    "plt.show()\n",
    "\n",
    "# Perform a sanity check on some random validation samples\n",
    "ix = random.randint(0, len(preds_val_t))\n",
    "imshow(image_dataset[int(image_dataset.shape[0]*0.9):][ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(mask_dataset[int(mask_dataset.shape[0]*0.9):][ix]))\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_val_t[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b01f28-0130-48dc-9c4a-00ae60a8dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(preds_test_t.shape)\n",
    "ix = random.randint(0, len(preds_test_t))\n",
    "imshow(test_dataset[ix])\n",
    "plt.show()\n",
    "imshow(np.squeeze(preds_test_t[ix]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b85482-8997-424a-b24a-57ed879adf88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "submission = []\n",
    "# The constants with _SIDE mean how many patches fit per image in one dimension (one side)\n",
    "TEST_IMAGE_LENGTH = 608\n",
    "PATCHES_PER_IMAGE_SIDE = math.ceil(TEST_IMAGE_LENGTH/PATCH_SIZE)\n",
    "PATCHES_PER_IMAGE = PATCHES_PER_IMAGE_SIDE**2\n",
    "SUBIMAGES_PER_PATCH_SIDE = PATCH_SIZE/16\n",
    "for i, pred in enumerate(preds_test):\n",
    "    img_id = test_ids[i//PATCHES_PER_IMAGE]\n",
    "    # Format the image id\n",
    "    id = img_id.split('_')[1].zfill(3)\n",
    "    # Make sure the patch size is a multiple of 16 otherwise this line won't work\n",
    "    preds = split_into_patches(pred, 16)\n",
    "    for j, img in enumerate(preds):\n",
    "        # Calculate the index of each subimage (in terms of pixels)\n",
    "        x = 16*(SUBIMAGES_PER_PATCH_SIDE*((i % PATCHES_PER_IMAGE) % PATCHES_PER_IMAGE_SIDE) + j % SUBIMAGES_PER_PATCH_SIDE)\n",
    "        y = 16*(SUBIMAGES_PER_PATCH_SIDE*((i % PATCHES_PER_IMAGE) // PATCHES_PER_IMAGE_SIDE) + j // SUBIMAGES_PER_PATCH_SIDE)\n",
    "        # Don't add the padding predictions\n",
    "        if x < TEST_IMAGE_LENGTH and y < TEST_IMAGE_LENGTH:\n",
    "            # For now we calculate the average over all the pixels and check if it's above 0.5\n",
    "            submission.append((f\"{id}_{x:.0f}_{y:.0f}\", 1 if img.mean() > 0.5 else 0))\n",
    "np.savetxt(\"predictions.csv\", np.asarray(submission), fmt=\"%s\", delimiter=\",\", newline=\"\\n\", header=\"id,prediction\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb1047",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
